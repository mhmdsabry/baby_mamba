[model_config]
block_size = 512
n_layer = 4
d_model = 768
d_state = 16
d_conv = 3
expand = 2   
dt_min = 0.001
dt_max = 0.1
dt_init = random
dt_scale = 1.0
dt_init_floor = 1e-4
conv_bias = True
in_out_proj_bias = False

[dataset]
dataset_path = ./tinyshakespeare.txt

[training_config]
max_epoch = 3
train_batch_size = 64
eval_batch_size = 32
num_workers = 4
learning_rate = 3e-4
weight_decay = 0.01
beta_1 = 0.9
beta_2 = 0.95
ckpt_path = ./mamba_ckpt

[generation_config]
num_generated_tokens = 200
generated_text_path = ./mamba_text